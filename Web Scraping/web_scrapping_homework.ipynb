{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "679c7fbb",
   "metadata": {},
   "source": [
    "#  News Sentiment Analysis with VADER + Google News RSS\n",
    "\n",
    "##  Objective\n",
    "In this notebook, you will collect recent news articles from Google News using an RSS feed, analyze their sentiment using NLTK‚Äôs VADER lexicon, and summarize your findings.\n",
    "\n",
    "You will:\n",
    "- Choose 3 topics of interest\n",
    "- Fetch recent headlines and summaries for each topic\n",
    "- Analyze their sentiment (positive, negative, neutral)\n",
    "- Create a summary table\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccb9fe8",
   "metadata": {},
   "source": [
    "## Step 0:  ‚Äì Scrape Headlines with `requests` + `BeautifulSoup`\n",
    "\n",
    "If you'd like to explore **another way to get news**, try scraping headlines directly from a news website.\n",
    "\n",
    "Choose a simple website like:\n",
    "- https://www.bbc.com/news\n",
    "- https://edition.cnn.com/\n",
    "- https://elpais.com/\n",
    "\n",
    "Use `requests` to download the HTML, and `BeautifulSoup` to extract the titles (look for `<h2>` or `<a>` tags inside news containers).\n",
    "\n",
    "Store at least 10 headlines in a list. You can later analyze these with the same sentiment pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74205f64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28ffa68e",
   "metadata": {},
   "source": [
    "## Step 1: Clean Headlines with NLP Tools\n",
    "\n",
    "Before analyzing the headlines, clean the text using NLP tools:\n",
    "\n",
    "- Lowercase all text\n",
    "- Tokenize into words\n",
    "- Remove stopwords (with `nltk` or `spacy`)\n",
    "- Lemmatize (optional but recommended)\n",
    "\n",
    "Show a sample of your cleaned tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4998e42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2078038a",
   "metadata": {},
   "source": [
    "##  Step 1.5: Check and Correct Spelling with SymSpell\n",
    "\n",
    "Using your scraped or fetched headlines, introduce a few **intentional spelling mistakes**, or analyze them as-is if they come from social media.\n",
    "\n",
    "Then:\n",
    "- Load the SymSpell dictionary\n",
    "- Detect and suggest corrections for at least 5 tokens\n",
    "- Display original and corrected terms\n",
    "\n",
    "Explain briefly if any real mistakes were found.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61751e03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "620737ed",
   "metadata": {},
   "source": [
    "## üåê Step 2: Fetch News from Google News RSS\n",
    "\n",
    "Use the helper function `fetch_news_items()` to retrieve at least 5 articles per keyword.\n",
    "\n",
    "Make sure each news item contains:\n",
    "- Title\n",
    "- Summary\n",
    "- Published date\n",
    "- Link\n",
    "\n",
    "Display the articles in a DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3869c23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c93e477e",
   "metadata": {},
   "source": [
    "## Step 3: Analyze Sentiment with VADER\n",
    "\n",
    "Use `SentimentIntensityAnalyzer` from NLTK to evaluate the sentiment of each article.\n",
    "\n",
    "You should:\n",
    "- Combine the title and summary\n",
    "- Compute the sentiment scores\n",
    "- Classify each article as:\n",
    "  -  positive (compound ‚â• 0.05)\n",
    "  -  negative (compound ‚â§ -0.05)\n",
    "  -  neutral (otherwise)\n",
    "\n",
    "Store the results in a DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544c2ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8136d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c13f18c5",
   "metadata": {},
   "source": [
    "##  Step 4: Summary by Keyword\n",
    "\n",
    "Group the results by keyword and create a summary that includes:\n",
    "- Mean compound score\n",
    "- Percentage of positive, negative, and neutral articles\n",
    "- Total number of articles\n",
    "\n",
    "You can use `groupby()` and `agg()` for this part.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabe6214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4488941b",
   "metadata": {},
   "source": [
    "##  Bonus: Keyword or Entity Recognition (Regex or FlashText)\n",
    "\n",
    "Use `FlashText` or `re.findall()` to search for keywords or patterns in your headlines or summaries.\n",
    "\n",
    "Some ideas:\n",
    "- Extract keywords like \"AI\", \"Bitcoin\", \"climate\"\n",
    "- Use regex to extract years (`\\d{4}`), dates, or email-like patterns\n",
    "\n",
    "Display the matches in a table or dictionary format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e645a178",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
